{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "06869db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                       MAIN KEYWORDS SUMMARY\n",
      "======================================================================\n",
      "Date: The second half (Count: 1)\n",
      "Date: every day (Count: 1)\n",
      "Org: Research Scientists (Count: 1)\n",
      "Cardinal: one (Count: 5)\n",
      "Org: DQLR (Count: 9)\n",
      "Cardinal: only half (Count: 1)\n",
      "Gpe: |1⟩. (Count: 1)\n",
      "Date: April 9, 2024 March 4, 2024 December 4, 2023 \n",
      "                    Follow (Count: 1)\n",
      "Cardinal: zero (Count: 1)\n",
      "Org: Google Quantum AI Team (Count: 1)\n",
      "Percent: about 0.1% (Count: 1)\n",
      "Org: Nature Physics (Count: 1)\n",
      "Percent: nearly 1% (Count: 1)\n",
      "Norp: MLR (Count: 1)\n",
      "Org: Quantum AI Team (Count: 1)\n",
      "Date: November 9, 2023 (Count: 1)\n",
      "Org: Leakage (Count: 3)\n",
      "Org: Google (Count: 2)\n",
      "Org: MLR (Count: 3)\n",
      "Cardinal: One (Count: 1)\n",
      "Person: Kevin Miao (Count: 1)\n",
      "Cardinal: fewer than two (Count: 1)\n",
      "Cardinal: two (Count: 14)\n",
      "Person: |0⟩. (Count: 1)\n",
      "Cardinal: Two (Count: 1)\n",
      "Person: Jenga (Count: 2)\n",
      "Org: quantum (Count: 4)\n",
      "Cardinal: billions (Count: 1)\n",
      "Person: |4⟩ (Count: 1)\n",
      "Cardinal: hundreds (Count: 1)\n",
      "Person: Matt McEwen (Count: 1)\n",
      "\n",
      "======================================================================\n",
      "                        OVERALL PAPER SUMMARY\n",
      "======================================================================\n",
      "Our research teams have the opportunity to impact technology used by billions of people every day. November 9, 2023 Posted by Kevin Miao and Matt McEwen, Research Scientists, Quantum AI Team \n",
      "The qubits that make up Google quantum devices are delicate and noisy, so it’s necessary to incorporate error correction procedures that identify and account for qubit errors on the way to building a useful quantum computer. Two of the most prevalent error mechanisms are bit-flip errors (where the energy state of the qubit changes) and phase-flip errors (where the phase of the encoded quantum information changes). Quantum error correction (QEC) promises to address and mitigate these two prominent errors. In “Overcoming leakage in quantum error correction”, published in Nature Physics, we identify when and how our qubits leak energy to higher states, and show that the leaked states can corrupt nearby qubits through our two-qubit gates.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Extract text from a web page\n",
    "def extract_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Extract text from paragraphs and join them\n",
    "    text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "    return text\n",
    "\n",
    "# Analyze text and Entities\n",
    "def analyze_text(text):\n",
    "    # Load spaCy's English tokenizer\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    # Extract named entities with their labels\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Extracting main keywords and Compose Summary Sentence\n",
    "def summarize_text(text, keywords, num_sentences=5):\n",
    "    # Tokenize text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    summary_sentences = []\n",
    "    # Check each sentence for the presence of keywords\n",
    "    for sentence in sentences:\n",
    "        # If a keyword is found in the sentence, add it to the summary\n",
    "        if any(keyword in sentence.lower() for keyword in keywords):\n",
    "            summary_sentences.append(sentence)\n",
    "            # If we have enough summary sentences, stop\n",
    "            if len(summary_sentences) >= num_sentences:\n",
    "                break\n",
    "    return ' '.join(summary_sentences)\n",
    "\n",
    "# Function to summarize named entities by their labels and include counts\n",
    "def summarize_entities(entities):\n",
    "    entity_dict = {}\n",
    "    for entity, label in entities:\n",
    "        entity_dict[(label, entity)] = entity_dict.get((label, entity), 0) + 1\n",
    "    return entity_dict\n",
    "\n",
    "# Function to summarize a web page, extracting main keywords and providing an overall summary\n",
    "def summarize_web_page(url, num_keywords=10, num_sentences=5):\n",
    "    # Extract text from the web page\n",
    "    text = extract_text(url)\n",
    "    # Analyze the text and find named entities\n",
    "    entities = analyze_text(text)\n",
    "    # Summarize named entities by their labels and include counts\n",
    "    entity_summary = summarize_entities(entities)\n",
    "    # Remove duplicates and count occurrences\n",
    "    unique_entities = {f\"{label.capitalize()}: {entity} (Count: {count})\" for (label, entity), count in entity_summary.items()}\n",
    "    # Extract main keywords\n",
    "    keywords = [entity for label, entity in entity_summary.keys()][:num_keywords]\n",
    "    # Summarize text using most frequent words and important sentences\n",
    "    summary = summarize_text(text, keywords, num_sentences)\n",
    "    \n",
    "    # Print the summary\n",
    "    print(\"======================================================================\")\n",
    "    print(\"                       MAIN KEYWORDS SUMMARY\")\n",
    "    print(\"======================================================================\")\n",
    "    for entity in unique_entities:\n",
    "        print(entity)\n",
    "    \n",
    "    print(\"\\n======================================================================\")\n",
    "    print(\"                        OVERALL PAPER SUMMARY\")\n",
    "    print(\"======================================================================\")\n",
    "    print(summary)\n",
    "\n",
    "# URL of the web page to explore\n",
    "url = 'https://research.google/blog/overcoming-leakage-on-error-corrected-quantum-processors/'\n",
    "\n",
    "# Summarize the web page\n",
    "summarize_web_page(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "325d419a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: python summarize_web_page.py <URL>\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Function to extract text from a web page\n",
    "def extract_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Extract text from paragraphs and join them\n",
    "    text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "    return text\n",
    "\n",
    "\n",
    "def analyze_text(text):\n",
    "    # Load spaCy's English tokenizer\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    # Extract named entities with their labels\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "def summarize_text(text, keywords, num_sentences=10):\n",
    "    # Tokenize text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    summary_sentences = []\n",
    "    # Check each sentence for the presence of keywords\n",
    "    for sentence in sentences:\n",
    "        # If a keyword is found in the sentence, add it to the summary\n",
    "        if any(keyword in sentence.lower() for keyword in keywords):\n",
    "            summary_sentences.append(sentence)\n",
    "            # If we have enough summary sentences, stop\n",
    "            if len(summary_sentences) >= num_sentences:\n",
    "                break\n",
    "    return ' '.join(summary_sentences)\n",
    "\n",
    "def summarize_entities(entities):\n",
    "    entity_counts = Counter(entities)\n",
    "    return entity_counts\n",
    "\n",
    "def summarize_web_page(url, num_keywords=5, num_sentences=10):\n",
    "\n",
    "    text = extract_text(url)\n",
    "\n",
    "    entities = analyze_text(text)\n",
    "\n",
    "    entity_summary = summarize_entities(entities)\n",
    "    # Extract main keywords\n",
    "    keywords = [entity for entity, _ in entity_summary.most_common(num_keywords)]\n",
    "    # Summarize text using most frequent words and important sentences\n",
    "    summary = summarize_text(text, keywords, num_sentences)\n",
    "    \n",
    "    # Print the summary\n",
    "    print(\"======================================================================\")\n",
    "    print(\"                       MAIN KEYWORDS SUMMARY\")\n",
    "    print(\"======================================================================\")\n",
    "    for entity, count in entity_summary.most_common(num_keywords):\n",
    "        print(f\"{entity.capitalize()} (Count: {count})\")\n",
    "    \n",
    "    print(\"\\n======================================================================\")\n",
    "    print(\"                        OVERALL PAPER SUMMARY\")\n",
    "    print(\"======================================================================\")\n",
    "    print(summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: python summarize_web_page.py <URL>\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    url = sys.argv[1]\n",
    "    # Summarize the web page\n",
    "    summarize_web_page(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b343ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
